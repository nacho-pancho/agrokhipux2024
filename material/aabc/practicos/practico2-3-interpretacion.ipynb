{"cells":[{"cell_type":"markdown","metadata":{"id":"aOaj9Na1fAJ_"},"source":["# Práctico 2-3: Interpretación de modelos\n","\n","\n","La interpretabilidad siempre es un elemento deseable de cualquier modelo de datos. Sea cual sea el objetivo, ser capaz de vincular los parámetros de un modelo aprendido con la naturaleza de los datos permite, entre otras cosas:\n","*   ganar conocimiento sobre el problema\n","*   mejorar la metodología de aprendizaje con ese conocimiento\n","*   diagnosticar posibles problemas de implementación\n","\n","Más allá de lo anterior, que puede verse como subproductos del proceso de aprendizaje, muchas veces el modelo aprendido es un fin en sí, más que un medio. Por ejemplo, si queremos diseñar una política pública para reducir el riesgo de cáncer en la población, podemos usar el resultado de una regresión para enfocar nuestras energías en las variables que resultaron ser más relevantes.\n","\n","No todos los modelos son fácilmente interpretables. Abajo veremos algunos que lo son, y algunas formas de  mejorar su interpretabilidad.\n","\n","## Regresión\n","\n","Como ya vimos, sea lineal o logística, la regresión vincula un conjunto de variables regresoras $X_1,X_2,\\ldots,X_m$ con una variable de respuesta $y$ mediante una relación lineal: $y= \\sum_{i=1}^m a_i X_i.$\n","\n","En estadística clásica, los parámetros del modelo, $a_1,\\ldots,a_m$ se llaman _cargas_ (loadings), denotando el peso relativo que tiene cada variable en la respuesta: claramente, mientras más grande $a_i$ (en valor absoluto), mayor su influencia en $y$; si $a_i=0$, la variable $X_i$ no incide en absoluto.\n","\n","Desafortunadamente, la forma tradicional de estimar los pesos $a_1,a_2,\\ldots,a_m$, el método de los _mínimos cuadrados_ no suele ser la más fiable ni la que arroja resultados más interpretables. Veamos esto en el problema de cancer de mama.\n","\n","El caso que vamos a ver abajo es un problema clásico, en este caso para regresión lineal. Se trata de predecir si un tumor de mama es maligno o no en base a una serie de medidas.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12919,"status":"ok","timestamp":1696869359525,"user":{"displayName":"Graciana Castro","userId":"14730492599317412530"},"user_tz":180},"id":"TQ7fO3mpmV4s","outputId":"44c1fbd5-d52f-4ff9-a7be-23330125ce56"},"outputs":[],"source":["!pip install --quiet mglearn\n","#\n","# importación\n","#\n","import mglearn\n","\n","import matplotlib as mpl\n","import matplotlib.cm as cm\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import numpy.random as rng\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","mpl.rcParams['figure.dpi'] = 100\n","mpl.rcParams['savefig.dpi'] = 150\n","\n","mpl.rcParams['font.size'] = 10\n","mpl.rcParams['legend.fontsize'] = 'medium'\n","mpl.rcParams['figure.titlesize'] = 'medium'\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2035,"status":"ok","timestamp":1696869361553,"user":{"displayName":"Graciana Castro","userId":"14730492599317412530"},"user_tz":180},"id":"pP8HMgvT9Ism","outputId":"afd18b88-f22d-4c8a-84b2-729afefd402a"},"outputs":[],"source":["#\n","# datos del problema\n","#\n","from sklearn import datasets\n","cancer = datasets.load_breast_cancer()\n","X = cancer.data\n","#\n","# normalización;\n","# esta forma de preprocesar los datos que suele\n","# ayudar mucho, sobre todo en métodos de regresión\n","# mas adelante veremos mas sobre esto\n","#\n","X -= np.mean(X,axis=0)\n","X /= np.std(X,axis=0)\n","\n","y = cancer.target\n","\n","print(\"=====================================\")\n","print(\"Datos del problema  : CANCER\")\n","print(\"numero de variables :\",X.shape[1])\n","print(\"numero de muestras  :\",X.shape[0])\n","print(\"=====================================\")\n","#\n","# dividimos en conjunto de entrenamiento y validación\n","#\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n","#\n","#  Regularización tipo Ridge (L2)\n","#\n","model = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n","coefs_l2 = model.coef_\n","train_score = model.score(X_train, y_train)\n","test_score  = model.score(X_test, y_test)\n","print(f\"LOGREG/L2   score train: {train_score:.2f} test: {test_score:.2f}\")\n","#\n","# Regularización tipo Lasso (L1)\n","#\n","model = LogisticRegression(solver='liblinear',penalty='l1').fit(X_train, y_train)\n","coefs_l1 = model.coef_.ravel()\n","train_score = model.score(X_train, y_train)\n","test_score  = model.score(X_test, y_test)\n","print(f\"LOGREG/LASSO score train: {train_score:.2f} test: {test_score:.2f}\")\n","\n","plt.figure(figsize=(8,10))\n","plt.subplot(2,1,1)\n","plt.plot(coefs_l2.T,'o')\n","plt.hlines(0, 0, cancer.data.shape[1])\n","plt.xticks(range(cancer.data.shape[1]), rotation=90)\n","plt.grid(True)\n","plt.ylim(-5, 5)\n","plt.title('Regresión logística con regularización tipo Ridge (L2)')\n","plt.xlabel(\"coeficiente\")\n","plt.ylabel(\"valor\")\n","\n","plt.subplot(2,1,2)\n","plt.plot(coefs_l1, 'o')\n","# pintamos los coeficientes exactamente 0  ('inactivos') de otro color:\n","inactivos = np.flatnonzero(coefs_l1 == 0)\n","plt.plot(inactivos,np.zeros(len(inactivos)),'o',color='gray')\n","plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n","plt.hlines(0, 0, cancer.data.shape[1])\n","plt.grid(True)\n","plt.ylim(-5, 5)\n","plt.title('Regresión logística con regularización tipo LASSO (L1)')\n","plt.xlabel(\"coeficiente\")\n","plt.ylabel(\"valor\")\n","#plt.legend()\n","plt.savefig('ridge_vs_lasso.png')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5E2feAbVB70Z"},"source":["## Comentarios\n","\n","Nota: podrían ser preguntas (todas o parte)\n","\n","*   Los desempeños son _idénticos_\n","*   Pero los coeficientes obtenidos son _muy_ distintos\n","*   Los coeficientes del caso $\\ell_2$ son todos no nulos\n","*   Los coeficientes del caso $\\ell_1$ son en su mayoría $0$: la regresión sólo depende de unas pocas variables\n","*   ¿Cuál es más interpretable?"]},{"cell_type":"markdown","metadata":{"id":"IU9REBhxClaJ"},"source":["# Importancia de las variables en árboles de decisión\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1636,"status":"ok","timestamp":1696869363177,"user":{"displayName":"Graciana Castro","userId":"14730492599317412530"},"user_tz":180},"id":"L3W9OjoPC1hJ","outputId":"bab853ff-a060-44b8-9401-3aea5834838f"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","model = DecisionTreeClassifier(max_depth=4, random_state=42)\n","model.fit(X_train, y_train)\n","train_score = model.score(X_train, y_train)\n","test_score  = model.score(X_test, y_test)\n","print(f\"ARBOL score train: {train_score:.2f} test: {test_score:.2f}\")\n","\n","#\n","# la importancia de cada variable se almacena aquí:\n","# la importancia de cada variable vale entre 0 y 1,\n","# siendo 0 = 'no importa' y 1 = 'determinante'\n","#\n","importances = model.feature_importances_\n","\n","\n","plt.figure(figsize=(8,10))\n","\n","plt.subplot(2,1,1)\n","plt.plot(importances, 'o')\n","inactivos = np.flatnonzero(importances == 0)\n","plt.plot(inactivos,np.zeros(len(inactivos)),'o',color='gray')\n","plt.xticks(range(cancer.data.shape[1]), rotation=90)\n","plt.grid(True)\n","plt.ylim(0,1)\n","plt.title('Importancia de variables en arbol de decisión')\n","plt.xlabel(\"coeficiente\")\n","plt.ylabel(\"importancia\")\n","\n","#\n","# improvisamos una importancia relativa para el caso de regresión logística\n","# usamos el valor absoluto de cada variable y dividimos todo por la suma\n","#\n","importancia_l1 = np.abs(coefs_l1)*(1/np.sum(np.abs(coefs_l1)))\n","\n","plt.subplot(2,1,2)\n","plt.plot(importancia_l1, 'o')\n","inactivos = np.flatnonzero(coefs_l1 == 0)\n","plt.plot(inactivos,np.zeros(len(inactivos)),'o',color='gray')\n","plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n","plt.ylim(0,1)\n","#plt.hlines(0, 0, cancer.data.shape[1])\n","plt.title('Regresión logística con regularización tipo LASSO (L1)')\n","plt.xlabel(\"coeficiente\")\n","plt.ylabel(\"importancia\")\n","plt.grid(True)\n","#plt.legend()\n","plt.savefig('tree_vs_lasso.png')\n","\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SkyN1BDHXhcR"},"source":["## Discusión\n","\n","En vista de la comparación de las importancias relativas de las variables usando Regresión Logística y Árboles:\n","\n","*   ¿Qué tan consistentes son las importancias asignadas por ambos métodos?\n","*   ¿Se le ocurre alguna manera de dar significancia estadística a estas medidas?\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1696869363178,"user":{"displayName":"Graciana Castro","userId":"14730492599317412530"},"user_tz":180},"id":"uclmsNF9S3KZ"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
