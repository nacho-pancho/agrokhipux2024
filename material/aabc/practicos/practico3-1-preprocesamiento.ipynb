{"cells":[{"cell_type":"markdown","metadata":{"id":"ovaZll-WnR7C"},"source":["# Práctico 3-1: Aprendizaje no supervisado\n","\n","\n","En este notebook veremos los distintos tipos de aprendizaje no supervisado más comunes:\n","\n","*   Preprocesamiento y aprendizaje de representaciones\n","*   Agrupamiento de datos\n","\n","Y algunos de los métodos más populares para lleva a cabo esta tarea.\n","\n","Al igual que en los notebooks anteriores, debemos ejecutar un pequeño preámbulo.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ux1Z9L7EnoaA"},"outputs":[],"source":["\n","#\n","# importación\n","#\n","import matplotlib as mpl\n","import matplotlib.cm as cm\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import numpy.random as rng\n","\n","mpl.rcParams['figure.dpi'] = 100\n","mpl.rcParams['savefig.dpi'] = 150\n","\n","mpl.rcParams['font.size'] = 10\n","mpl.rcParams['legend.fontsize'] = 'medium'\n","mpl.rcParams['figure.titlesize'] = 'medium'\n","\n","from sklearn import datasets\n","\n","cancer = datasets.load_breast_cancer()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rj8HR4pR1WOE"},"source":["## Preprocesamiento\n","\n","Desde la adquisición hasta su uso para hacer inferencia, los datos usualmente deben ser acondicionados de alguna forma. A esto le llamamos _preprocesamiento_.\n","No es necesariamente un tipo de aprendizaje, si bien en general implica aprender o estimar alguna que otra estadística, o incluso realizar algún tipo de inferencia a partir de los datos.\n","Por ejemplo, es común que haya datos faltantes o incompletos.\n","También es común que haya datos anómalos, medidas erróneas debidas a equipos defectuosos, alguien que anotó mal en una planilla, etc..\n","\n","En otros casos sucede que los propios métodos de ajuste de modelo son sensibles a ciertos aspectos, en principio irrelevantes, de los datos, a veces por cuestiones meramente técnicas. El ejemplo más común de esto es el _centrado_ y la _normalización_.\n","\n","### Centrado\n","\n","Se trata de restarle a todas las muestras de un conjunto de datos su valor medio. Si tenemos datos de entrenamiento $(\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n)$, se calcula\n","\n","$$ \\bar{\\mathbf{x}} =  \\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{x}_j$$\n","\n","y luego se transforma a todos los datos de entrenamiento mediante $\\mathbf{x}_j \\leftarrow \\mathbf{x}_j - \\bar{\\mathbf{x}}$.\n","\n","Notar que aquí cada dato $\\mathbf{x}_j$ es un vector de $m$ dimensiones, donde cada dimensión $i$ corresponde a una característica. Para referirnos al elemento $i$-ésimo del dato $j$-ésimo escribimos $x_{ij}$.\n","\n","\n","### Normalización\n","\n","Hay muchas variantes de este proceso, pero todas tienen el mismo objetivo: hacer que el rango de variación de las distintas variables en las muestras sea más o menos el mismo, y en torno a un mismo rango, típicamente $[0,1]$ o bien $[-1,1]$. La forma más común de hacer esto es calcular la desviación empírica en cada dimensión $i$ de los datos:\n","\n","$$\\sigma_i = \\sqrt{(\\sum_{j=1} x_{ij}-\\bar{x}_j)^2}$$\n","\n","y luego dividir los datos en la dimensión $i$ de manera que su desviación a lo largo del conjunto de entrenamiento pase a ser $1$: $x_{ij} \\leftarrow x_{ij} / \\sigma_i$.  A este tipo de normalización se le llama _estandarización_.\n","\n","Otra muy común consiste en _estirar_ o _encoger_ los datos en cada dimensión de manera que el mínimo sea $0$ (puede ser $-1$) y el máximo sea $1$.\n","\n","Veamos cómo realizar estas operaciones a mano. El paquete `sklearn` ya incluye todas estas operaciones, y se recomienda usar sus funciones siempre que sea posible, para no introducir errores innecesarios.\n","\n","Por esta vez, por motivos didácticos, veremos cómo hacerlas a mano:\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":877},"executionInfo":{"elapsed":1257,"status":"ok","timestamp":1694456822334,"user":{"displayName":"Graciana Castro","userId":"14730492599317412530"},"user_tz":180},"id":"JqDGXeWT3jqx","outputId":"31dd6050-4eb3-4c66-d14f-edd1eec9c728"},"outputs":[],"source":["import numpy as np\n","import numpy.random as random\n","import matplotlib.pyplot as plt\n","#\n","# generamos un dataset al azar\n","#\n","m = 2\n","n = 30\n","random.seed(1234)\n","#\n","# matriz de datos: cada fila es un dato\n","# cada dato es un vector de dimensión m=2\n","#\n","plt.figure(figsize=(10,10))\n","plt.subplot(2,2,1)\n","X = [0.3,0.5] + np.dot(random.randn(n,m),np.diag([0.2,0.4]))\n","y = random.rand(n) > 0.5\n","plt.scatter(X[:,0],X[:,1],c=y)\n","plt.grid(True)\n","plt.xlabel('x_1')\n","plt.ylabel('x_2')\n","plt.ylim(-2,2)\n","plt.xlim(-2,2)\n","plt.title('Datos originales')\n","\n","#\n","# centramos:\n","#\n","plt.subplot(2,2,2)\n","xbar = np.mean(X,axis=0) # tomamos el valor medio de cada columna\n","print('valor medio ',xbar)\n","X = X - xbar\n","\n","plt.scatter(X[:,0],X[:,1],c=y)\n","plt.grid(True)\n","plt.xlabel('x_1')\n","plt.ylabel('x_2')\n","plt.ylim(-2,2)\n","plt.xlim(-2,2)\n","plt.title('Datos centrados')\n","#\n","# estandarización\n","#\n","sigma = np.std(X,axis=0) # tomamos la desviación emírica de cada columna\n","print('desviacion de cada dimension ',sigma)\n","X = X / sigma\n","\n","plt.subplot(2,2,3)\n","plt.scatter(X[:,0],X[:,1],c=y)\n","plt.grid(True)\n","plt.ylim(-2,2)\n","plt.xlim(-2,2)\n","plt.xlabel('x_1')\n","plt.ylabel('x_2')\n","plt.title('Datos centrados y normalizados')\n","\n","#\n","# estiramiento de rango\n","#\n","xmin = np.min(X,axis=0)\n","xmax = np.max(X,axis=0)\n","X = (X-xmin)/(xmax-xmin)\n","\n","plt.subplot(2,2,4)\n","plt.scatter(X[:,0],X[:,1],c=y)\n","plt.grid(True)\n","plt.ylim(-0.1,1.1)\n","plt.xlim(-0.1,1.1)\n","plt.xlabel('x_1')\n","plt.ylabel('x_2')\n","plt.title('Datos estirados')\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FdbD6K1hIpk9"},"source":["## Preprocesamiento como forma de mejorar el entrenamiento\n","\n","Como mencionamos antes, una de las principales razones para efectuar un preprocesamiento como el anterior es que, debido a cuestiones técnicas pero no por ello menores, muchos métodos de aprendizaje dependen de esta etapa para producir un buen resultado, como veremos a continuación.\n","\n","De paso, vamos a usar las funciones de preprocesamiento que ya vienen incorporadas en `sklearn`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1475,"status":"ok","timestamp":1632832893701,"user":{"displayName":"Ivan Meresman Higgs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtoDg2ylo0GCeSYJ0IS8owqSGaowutXNnJlTJEcA=s64","userId":"10619444747042131358"},"user_tz":180},"id":"SFOeuoLG1aQ1","outputId":"e224d060-3e6e-41a6-afda-29d5dc4e8f7a"},"outputs":[],"source":["\n","from sklearn import datasets\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC # support vector classifier\n","\n","cancer = datasets.load_breast_cancer()\n","X = cancer.data\n","y = cancer.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n","#\n","# entrenamos y probamos con datos _no_ preprocesados\n","#\n","model = SVC(C=100)\n","model.fit(X_train,y_train)\n","print(\"score (sin preprocesamiento):\",model.score(X_test,y_test))\n","\n","#\n","# entrenamos y probamos con datos preprocesados\n","#\n","preprocessor = MinMaxScaler().fit(X_train)\n","X_train_pre  = preprocessor.transform(X_train)\n","X_test_pre   = preprocessor.transform(X_test)\n","\n","model = SVC(C=100)\n","model.fit(X_train_pre,y_train)\n","print(\"score (con preprocesamiento):\",model.score(X_test_pre,y_test))\n","#\n","# NOTA: en el libro da una diferencia _dramática_ que no se observa aquí\n","# incluso con exactamente los mismos parámetros.\n","#\n","\n","\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
