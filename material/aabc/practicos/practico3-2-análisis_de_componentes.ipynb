{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Práctico 3-2: Análisis de Componentes Principales (PCA)\n","\n","Este tema está entre medio del preprocesamiento y el aprendizaje.\n","\n","El análisis de componentes principales (Principal Component Analysis) es una herramienta fundamental de amplio uso en el análisis de datos desde hace más de 100 años! Hoy en día, con las facilidades computacionales de las que disponemos, es una técnica accesible y muy útil.\n","\n","Para no entrar en detalles matemáticos, que no son triviales, veámoslo con el siguiente ejemplo:\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":613,"status":"ok","timestamp":1696957708701,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"ux1Z9L7EnoaA"},"outputs":[],"source":["\n","#\n","# importación\n","#\n","import matplotlib as mpl\n","import matplotlib.cm as cm\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import numpy.random as rng\n","\n","mpl.rcParams['figure.dpi'] = 100\n","mpl.rcParams['savefig.dpi'] = 150\n","\n","mpl.rcParams['font.size'] = 10\n","mpl.rcParams['legend.fontsize'] = 'medium'\n","mpl.rcParams['figure.titlesize'] = 'medium'\n","\n","from sklearn import datasets\n","\n","cancer = datasets.load_breast_cancer()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2VKvvdCt1aa_"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"elapsed":1807,"status":"ok","timestamp":1696957710505,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"YxkSybEZ9JSc","outputId":"f4018aaa-d7f0-458a-ab93-403ad18ddf9e"},"outputs":[],"source":["#\n","# generamos datos sintéticos\n","#\n","def cigarro(n=1000):\n","  '''\n","  esta función genera una nube de puntos alargada\n","  '''\n","  X = rng.randn(n,2)\n","  y = rng.rand(n) > 0.50\n","  A = np.array([[0.5,-0.4],[-0.2,0.2]])\n","  X = np.dot(X,A)\n","  return X,y\n","#\n","# llamamos a la función\n","#\n","X,y = cigarro()\n","plt.figure(figsize=(5,5))\n","plt.scatter(X[:,0],X[:,1],color=(0,0,0,0.2))\n","plt.xlim(-1.2,1.2)\n","plt.ylim(-1.2,1.2)\n","plt.xlabel('x_1')\n","plt.ylabel('x_2')\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"tJEigLxp-0HD"},"source":["Los datos anteriores se distribuyen en el plano de una manera muy particular: claramente hay una _dirección principal_a lo largo de la cual hay más _variación_ en los datos. En la dirección perpendicular los puntos están muy apretados.\n","\n","Si por alguna razón sólo pudiéramos  analizar estos datos en una dirección sin otra información que la distribución espacial de los puntos, sin dudas lo haríamos en la dirección de mayor variación. De eso se trata PCA.\n","\n","PCA efectúa una _transformación_ de los datos, un _cambio de coordenadas_, de manera que, en el nuevo sistema de coordenadas, la primera coordenada es la de mayor variación y la última es la de menos variación. De esa manera, si tenemos que restringir nuestro análisis a unas pocas coordenadas, nos quedamos sólo con las primeras.\n","\n","En el ejemplo anterior, si tenemos dos direcciones y queremos quedarnos sólo con la primera, hacemos lo siguiente:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":508},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1696957710505,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"aguZ-g2VAouP","outputId":"e1bf451e-c18a-4816-c22f-cf9c135c6444"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","\n","#\n","#  crucial en PCA: centramos los datos primero!\n","#\n","mu = np.mean(X,axis=0)\n","X_center = X - mu\n","#\n","# calculamos las direcciones principales\n","#\n","pca = PCA(n_components=2)\n","pca.fit(X_center)\n","\n","#\n","# mostramos los componentes\n","#\n","c1 = pca.components_[0,:]\n","c2 = pca.components_[1,:]\n","\n","plt.figure(figsize=(5,5))\n","plt.scatter(X[:,0],X[:,1],color=(0,0,0,0.2))\n","a = mu - c1\n","b = mu + c1\n","plt.plot((a[0],b[0]),(a[1],b[1]),'r',lw=2)\n","a = mu - c2\n","b = mu + c2\n","plt.plot((a[0],b[0]),(a[1],b[1]),'b',lw=2)\n","\n","plt.xlim(-1.2,1.2)\n","plt.ylim(-1.2,1.2)\n","plt.xlabel('x_1')\n","plt.ylabel('x_2')\n","plt.grid(True)\n","plt.title('Componentes principales')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":508},"executionInfo":{"elapsed":1426,"status":"ok","timestamp":1696957711925,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"cvwXxAbVDeyk","outputId":"f53ef548-5bc8-412e-e3b4-c4b444497361"},"outputs":[],"source":["#\n","# representamos (transformamos) nuestros datos en las nuevas coordenadas\n","#\n","plt.figure(figsize=(5,5))\n","X_pca = pca.transform(X_center)\n","plt.scatter(X_pca[:,0],X_pca[:,1],color=(0,0,0,0.2))\n","plt.xlim(-1.2,1.2)\n","plt.ylim(-1.2,1.2)\n","plt.title('Datos transformados')\n","plt.xlabel('componente de mayor variabilidad')\n","plt.ylabel('componente de menor variabilidad')\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"S52Q207iDq_u"},"source":["## Observación\n","\n","Como podemos ver, luego de transformados, los datos están \"acostados\" sobre el eje principal (el eje 'x'). Ahora vamos a ver cómo aplicar esta técnica para visualizar datos de altas dimensiones en 2D.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":510},"executionInfo":{"elapsed":6817,"status":"ok","timestamp":1696957718728,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"veokY1w8D92y","outputId":"8f592317-bad1-4566-a46c-e6f604d7ae54"},"outputs":[],"source":["!pip install --quiet mglearn\n","\n","import mglearn\n","from sklearn import datasets\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","#\n","# datos a analizar: cancer de mama\n","#\n","cancer = datasets.load_breast_cancer()\n","X = cancer.data\n","y = cancer.target\n","print('NOTA: Los datos originales tienen',X.shape[1],'dimensiones')\n","print('NOTA: vamos a quedarnos sólo con DOS!')\n","#\n","# centramos y normalizamos los datos\n","#\n","preproc = StandardScaler()\n","preproc.fit(X)\n","X_scaled = preproc.transform(X)\n","#\n","# calculamos los ejes principales\n","# sólo nos interesa preservar DOS, ya que nuestro objetivo\n","# es visualizar los datos en pantalla\n","#\n","pca = PCA(n_components=2)\n","pca.fit(X_scaled)\n","#\n","# proyectamos (transformamos) los datos sobre\n","# los dos ejes principales calculados\n","#\n","X_pca = pca.transform(X_scaled)\n","#\n","# mostramos la nube de puntos proyectada sobre los ejes\n","# principales\n","#\n","plt.figure(figsize=(5, 5))\n","mglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\n","plt.legend(cancer.target_names, loc=\"best\")\n","plt.gca().set_aspect(\"equal\")\n","plt.xlabel(\"Primer componente principal\")\n","plt.ylabel(\"Segundo componente principal\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ULTumZFfFi7y"},"source":["### Comentarios\n","\n","Como puede verse, con sólo dos dimensiones alcanza para separar buena parte de los datos correspondientes a tumores malignos de los benignos. Como primera aproximación al problema, esto nos indica que hay esperanzas de automatizar este proceso con cierto éxito.\n","\n","Es importante recordar que PCA es un método _no supervisado_. No toma en cuenta los datos de clase a la hora de hacer la transformación. Hay otros métodos que sí lo hacen (pero no vamos a entrar en ellos ahora).\n","\n","Desafortunadamente, hay un trecho entre  tener una representación visual de los datos, y tener un modelo interpretable. En general, los ejes principales de PCA son combinaciones caprichosas y poco informativas de las coordenadas originales, es decir, de las características originales; las componentes en sí no significan nada.\n","\n","## PCA para extracción de características\n","\n","Ya hemos mencionado que una etapa importante de un sistema de aprendizaje automático es lograr una representación compacta y sucinta de los datos, de manera de facilitar las etapas de decisión posteriores. Este mismo tema, conocido como _extracción de características_, _feature extraction_ o _feature engineering_, será tratado especialmente en el próximo módulo.\n","\n","En este momento nos limitaremos a comentar que PCA es una muy buena forma de resumir datos: debido a que es capaz de concentrar la variablidad de lo datos en unos pocos componentes, las coordenadas en estos componentes son candidatas naturales a ser usadas como características. Veamos en este mismo ejemplo qué pasa si queremos clasificar tumores usando componentes principales.\n","\n","Lo que vamos a hacer es lo siguiente: por un lado vamos a tomar las $30$ características originales y vamos a construir 6 modelos lineales con las primeras 1, 5, 10, 15, 20 y 25.\n","\n","Luego vamos a hacer lo mismo pero con las coordenadas de los datos sobre los ejes principales.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":573,"status":"ok","timestamp":1696957719292,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"GafXfmn7IbQq","outputId":"54ffe706-ba68-4789-c5fc-fc7d497d1cd4"},"outputs":[],"source":["from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","\n","X_train,X_test,y_train,y_test = train_test_split(X_scaled,y,random_state=30)\n","\n","pca = PCA(n_components=30)\n","pca.fit(X_train)\n","X_train_pca = pca.transform(X_train)\n","X_test_pca  = pca.transform(X_test)\n","\n","for d in (1,5,10,15,20,25):\n","  model_a = SVC(C=10).fit(X_train[:,:d],y_train)\n","  model_b = SVC(C=10).fit(X_train_pca[:,:d],y_train)\n","  score_a = model_a.score(X_test[:,:d],y_test)\n","  score_b = model_b.score(X_test_pca[:,:d],y_test)\n","  print(f'componentes:{d:2} sin PCA:{score_a:4.2f} con PCA:{score_b:4.2f}')\n"]},{"cell_type":"markdown","metadata":{"id":"XpgMhw_RKp5l"},"source":["### Comentarios\n","\n","Como podemos ver, usando PCA logramos un desempeño casi máximo con tan sólo $5$ componentes, mientras que sin usar PCA necesitamos al menos $15$.\n","\n","Como segunda observación, pero no menos importante: vemos que usando $10$ componentes principales logramos un _mejor_ desempeño que usando $25$! Por qué? Porque el modelo tiene menos parámetros y por ende resulta más adecuado para la cantidad de datos (relativamente poca) que hay en este dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"4axq8TRQMz5Q"},"source":["# Factorización de matrices\n","\n","La buśqueda de explicaciones simples es un concepto muy poderoso en la filosofía de la ciencia y no es de extrañar que muchos métodos de análisis de datos se basen en ese principio.\n","\n","En regresión por ejemplo, métodos como el LASSO buscan coeficientes lineales que sean en su mayoría cero; de esa manera, la variable de interés puede _explicarse_ como función de un conjunto pequeño de factores.\n","\n","Los métodos de factorización matricial persiguen un objetivo similar. Dada una matriz $\\mathbf{X}$ de tamaño $n{\\times}m$, la idea es reescribir dicha matriz como\n","$$\\mathbf{X} = \\mathbf{U}\\mathbf{V} + \\mathbf{E},$$\n","donde $\\mathbf{U}$ tiene tamaño $n{\\times}k$, $\\mathbf{V}$  $k{\\times}m$, y $\\mathbf{E}$ es del mismo tamaño que $\\mathbf{X}$.\n","\n","Cabe recordar que\n","$$\\mathbf{U}\\mathbf{V} = \\sum_{r=1}^{k}\\mathbf{U}[r,:] \\mathbf{V}[:,r]$$\n","A cada par de fila $\\mathbf{U}[r,:]$ y columna $\\mathbf{V}[:,r]$ lo llamamos _factor_.\n","\n","La gracia es lograr lo anterior con dos condiciones:\n","\n","*   $k \\ll m$ (pocos factores)\n","*   $\\mathbf{E} \\approx \\mathbf{0}$ (poco error de aproximación)\n","\n","\n","Dicho de otra forma, queremos _aproximar_ $\\mathbf{X}$ como la suma de $k$ factores.\n","\n","Ahora, si $\\mathbf{X}$ es una matriz de datos tal como venimos viendo, donde $n$ es la cantidad de muestras y $m$ es la dimensión de cada muestra, lo que queremos es _representar_  nuestros datos de dimensión $m$ como la suma de unos pocos factores.\n","\n","## PCA como factorización de matrices\n","\n","El método PCA que vimos antes es de hecho una forma, _la_ forma más común de factorizar matrices: las  filas de $\\mathbf{V}$ son los _componentes_, y las columnas de $\\mathbf{U}$ expresan el peso relativo de cada componente en cada una de las muestras de los datos. La gracia de PCA es que los factores se calculan de modo que los primeros factores son más importantes que los siguientes, lo que nos permite descartar factores arriba de un cierto valor $k$ y aún así mantener una buena representación ($\\mathbf{E} \\approx 0$).\n","\n","## NMF\n","\n","Hay muchísimos métodos de factorización de matrices. Entre los más populares están NMF: _Non-negative Matrix Factorization_. La idea es hacer algo similar a PCA, imponiendo que los factores y sus pesos sean _no negativos_. Esto a veces es importante cuando lo que se quiere explicar son fenómenos multicausales en donde cada _causa_ (factor) sólo puede _sumar_, nunca _restar_ al resultado final.\n","\n","## Identificación de personas\n","\n","Vamos a ver ahora el ejemplo utilizado en el libro para ilustrar el método NMF, y vamos a compararlo con PCA. Se trata de identificar personas a partir de fotos de su rostro.\n","\n","En este caso, cada fila de la matriz de datos $\\mathbf{X}$ es una imagen (_desenrollada_, con todos los pixeles en una sóla fila).\n","\n","El tema aquí es que la imagen, como dato, tiene una dimensión _enorme_. Veremos que no es buena idea comparar imágenes pixel a pixel; es una comparación muy burda y muy sensible a cambios de iluminación o punto de vista, por dar algunos ejemplos.\n","\n","En este caso también hay que hacer un poco de preprocesamiento. Resulta que la base de datos de caras tiene demasiados ejemplos de algunas personas; no es balanceado. Lo primero que vamos a hacer entonces es obtener una muestra que sea lo más equitativa de cáda persona en la base.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":720},"executionInfo":{"elapsed":41863,"status":"ok","timestamp":1696957761151,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"xeCp_ju-vyPR","outputId":"b394818b-f3de-4209-a564-007cfff4b223"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","#\n","# base de datos de personas\n","#\n","people = datasets.fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n","image_shape = people.images[0].shape\n","#\n","# visualización de los datos\n","#\n","fix, axes = plt.subplots(2, 5, figsize=(15, 8),subplot_kw={'xticks': (), 'yticks': ()})\n","for target, image, ax in zip(people.target, people.images, axes.ravel()):\n","  ax.imshow(image)\n","  ax.set_title(people.target_names[target])\n","plt.show()\n","#\n","# PREPROCESAMIENTO: balanceo\n","#\n","# tomamos un máximo de 50 caras por cada individuo\n","#\n","mask = np.zeros(people.target.shape, dtype=np.bool)\n","for target in np.unique(people.target):\n","  mask[np.where(people.target == target)[0][:50]] = 1\n","X_people = people.data[mask]\n","y_people = people.target[mask]\n","#\n","# PREPROCESAMIENTO: normalización\n","#\n","# normalizamos los datos para que el valor máximo de pixel sea 1\n","# y el menor 0.\n","# las imágenes suelen tener valores de entre 0 y 255, así que\n","# dividimos todo por 255\n","#\n","X_people = X_people / 255.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":683,"status":"ok","timestamp":1696957761828,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"ZB8s0hOr01U4","outputId":"d011da44-084b-45ac-e5df-790f7798bb20"},"outputs":[],"source":["#\n","# PRIMERA PRUEBA\n","#\n","# veamos qué pasa si intentamos clasificar los rostros\n","# por cercanía en el espacio original.\n","# usaremos el ya conocido método de vecinos más cercanos\n","# usando UN sólo vecino como referencia.\n","#\n","from sklearn.neighbors import KNeighborsClassifier\n","#\n","# dividimos los datos en entrenamiento y prueba\n","#\n","X_train, X_test, y_train, y_test = train_test_split(X_people, y_people, stratify=y_people, random_state=0)\n","#\n","# construimos el clasificador K-NN\n","#\n","knn = KNeighborsClassifier(n_neighbors=1)\n","knn.fit(X_train, y_train)\n","#\n","#\n","print(\"\\n===================================\")\n","print(\"1-NN sobre espacio original:\")\n","print(\"\\tDimensión:\",X_test.shape[1])\n","print(\"\\tscore {:.2f}\".format(knn.score(X_test, y_test)))\n","print(\"===================================\\n\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":980},"executionInfo":{"elapsed":3383,"status":"ok","timestamp":1696957765208,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"K4Ovg-nS04zQ","outputId":"d5acf8ec-ecfb-4cc6-fb97-a5059066bb24"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","#\n","# descomponemos los datos en 100 componentes usando PCA\n","# como vimos, lo que hace PCA por defecto es _rotar_ los datos\n","# de manera que los ejes se correspondan con direcciones de máxima variación\n","# en esta ocasión vamos a habilitar la opción _whiten_, que además de rotar,\n","#  normaliza los datos una vez transformados (como vimos al ppio de este notebook)\n","#\n","pca = PCA(n_components=100,whiten=True)\n","pca.fit(X_train)\n","X_train_pca = pca.transform(X_train)\n","X_test_pca  = pca.transform(X_test)\n","#\n","# vemos los 15 componentes como imágenes\n","# estos componentes no son imágenes comunes, ya que tienen pixeles _negativos_\n","# algo que no tiene sentido físico.\n","#\n","fix, axes   = plt.subplots(3, 5, figsize=(15, 12),subplot_kw={'xticks': (), 'yticks': ()})\n","for i ,(component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n","  ax.imshow(component.reshape(image_shape))\n","  ax.set_title(f\"componente no. {i}\")\n","  if i == 15:\n","    break\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":76858,"status":"ok","timestamp":1696957842060,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"-gcwU5_y1SGP","outputId":"7d8c8a75-2994-4d42-e2f0-f751a86a0755"},"outputs":[],"source":["from sklearn.decomposition import NMF\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","#\n","# ahora hacemos la descomposición pero con 15 componentes no-negativos\n","# usando NMF.\n","# hay que tener en cuenta que NMF es un _tipo_ de descomposición; hay\n","# varios métodos distintos que hacen NMF\n","#\n","nmf = NMF(n_components=100, random_state=0)\n","nmf.fit(X_train)\n","X_train_nmf = nmf.transform(X_train)\n","X_test_nmf  = nmf.transform(X_test)\n","#\n","# mostramos los componentes como imágenes\n","#\n","fix, axes   = plt.subplots(3, 5, figsize=(15, 12),\n","subplot_kw  = {'xticks': (), 'yticks': ()})\n","for i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\n","  ax.imshow(component.reshape(image_shape))\n","  ax.set_title(f\"componente no. {i}\")\n","  if i == 15:\n","    break\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":487,"status":"ok","timestamp":1696957842537,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"R0y7z_ZazNID","outputId":"123403af-4723-474a-d4d8-b8eee9ccd59c"},"outputs":[],"source":["#\n","# nuevamente clasificamos, ahora con los componentes\n","# obtenidos con cáda método\n","#\n","#\n","# repetimos método naïve, usando los datos originales\n","# (espacio de dimensión 5655)\n","#\n","knn_naive = KNeighborsClassifier(n_neighbors=2)\n","knn_naive.fit(X_train, y_train)\n","score_naive = knn_naive.score(X_test, y_test)\n","#\n","#\n","# ahora hacemos lo mismo pero con los 100 componentes elegidos con PCA\n","# (espacio de dimensión 100)\n","#\n","knn_pca = KNeighborsClassifier(n_neighbors=2)\n","knn_pca.fit(X_train_pca, y_train)\n","score_pca = knn_pca.score(X_test_pca, y_test)\n","#\n","#\n","# y finalmente pero con los 100 componentes elegidos por NMF\n","# (espacio de dimensión 100)\n","#\n","knn_nmf = KNeighborsClassifier(n_neighbors=2)\n","knn_nmf.fit(X_train_nmf, y_train)\n","score_nmf = knn_pca.score(X_test_nmf, y_test)\n","#\n","# comparamos los resultados\n","#\n","print(\"\\n===================================\")\n","print(f\"naive: score {score_naive:.2f}\")\n","print(f\"PCA  : score {score_pca:.2f}\")\n","print(f\"NMF  : score {score_nmf:.2f}\")\n","print(\"===================================\\n\")\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1696957842538,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"rkwAkLYe4T_-"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
