{"cells":[{"cell_type":"markdown","metadata":{"id":"4ubr1U7IAVDn"},"source":["# Práctico 2-1: Regresión\n","\n","## Preparación del entorno"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7743,"status":"ok","timestamp":1696351513245,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"wKKQ--i1-zm2","outputId":"32c7d715-83fc-4077-ee93-495454b13d89"},"outputs":[],"source":["\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","\n","mpl.rcParams['figure.figsize'] = [5.0, 4.0]\n","mpl.rcParams['figure.dpi'] = 100\n","mpl.rcParams['savefig.dpi'] = 150\n","mpl.rcParams['font.size'] = 11\n","mpl.rcParams['legend.fontsize'] = 'medium'\n","mpl.rcParams['figure.titlesize'] = 'medium'\n"]},{"cell_type":"markdown","metadata":{"id":"ZhvnFJ8uAk6R"},"source":["# Regresión\n","\n","En sentido general, el término _regresión_ refiere al problema clásico de estadística de _estimar relaciones_ entre un conjunto de variables $X_1,X_2,\\ldots,X_m$ y una respuesta $y$.\n","\n","En el contexto de aprendizaje automático, esto se considera un caso particular del problema de aprendizaje supervisado, y como tal suele ser tratado dentro de este tema.\n","\n","## Problema de ejemplo\n","\n","A continuación veremos varios métodos de regresión aplicados a un mismo problema sintético similar al que utiliza el libro en el Capítulo 2.\n","Concretamente, vamos a tomar muestras _ruidosas_ de una _onda sinusoidal_, es decir, nuestras observaciones van a ser de la forma $y = \\sin x + z$, donde $z$ es ruido Gaussiano de media nula $\\mu=0$ y desviación $\\sigma=0.3$. En la celda de código de abajo vemos cómo definir una función que genere estos datos, tomamos algunas muestras y las dibujamos:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":669,"status":"ok","timestamp":1632234116502,"user":{"displayName":"Ivan Meresman Higgs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtoDg2ylo0GCeSYJ0IS8owqSGaowutXNnJlTJEcA=s64","userId":"10619444747042131358"},"user_tz":180},"id":"xim5vcWp_VFE","outputId":"4fcc5821-eb06-4f83-ead5-f7aa960a4089"},"outputs":[],"source":["import numpy as np\n","import numpy.random as rng\n","\n","def onda_ruidosa(num_muestras,amplitud=1,frecuencia=1,fase=0,ruido=0,semilla=44441111):\n","  '''\n","  genera muestras de una onda sinusoidal de amplitud, frecuencia y fase\n","  deseadas, con una cantidad de ruido específica\n","  '''\n","  rng.seed(semilla)\n","  X = np.ones((num_muestras,1))\n","  X = np.linspace(0,2*np.pi,num=num_muestras)\n","  y = amplitud*np.sin(frecuencia*X+fase) + ruido*rng.randn(num_muestras)\n","  X = X.reshape(1,-1).T # detalle técnico oscuro y que no aporta nada\n","  return X,y\n","\n","#\n","# dataset sintético para regresión\n","#\n","N = 30\n","X, ytilde = onda_ruidosa(num_muestras=N,ruido=0.0)\n","X, y      = onda_ruidosa(num_muestras=N,ruido=0.3)\n","\n","plt.plot(X, ytilde, '-',color=(0.5,0.5,0.5,1.0))\n","plt.plot(X, y, 'o')\n","plt.ylim(-2, 2)\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.title(\"Onda sinusiodal ruidosa\")\n","plt.grid(True)\n","plt.savefig('onda_ruidosa.png')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"sYSVMHVxON0l"},"source":["## Regresión lineal\n","\n","Lo primero que veremos son regresiones lineales. En este caso, la relación a estimar es de la forma $$Y(X) = a_1 X_1 + a_2 X_2 + \\ldots + a_m X_m + b.$$\n","Lo que se _aprende_ en este caso son los _coeficientes_ $a_1,a_2,\\ldots,a_m$ y $b$.\n","\n","### Mínimos cuadrados\n","\n","La manera más tradicional y simple de realizar dicha estimación es mediante el método de _mínimos cuadrados_ (LS, por _least squares_). Dado un conjunto de entrenamiento compuesto por $n$ pares _entrada-salida_ $(x_j,y_j)$, $j=1,\\ldots,n$, los coeficientes LS se calculan de forma que la siguiente suma de _residuos cuadráticos_ sea mínima:\n","\n","$$\\sum_{j=1}^n \\left[ \\left( \\sum_{i=1}^m a_ix_{ji} + b\\right) - y_j \\right]^2$$\n","\n","El problema presentado por este método tiene una solución muy simple y cualquier software o paquete de estadística es capaz de calcularla. En la siguiente celda vemos cómo utilizar las funciones de `sklearn` para realizar esta tarea en un caso sintético unidimiensional. Aquí $X$ es una sóla variable y lo que tenemos que estimar son sólo dos parámetros:\n","\n","$$y = ax + b.$$\n","\n","## División entre conjunto de entrenamiento y validación\n","\n","Aquí vemos un primer ejemplo de cómo validar un modelo. Nuestro objetivo es ver qué tan bien se ajusta nuestro modelo a _nuevos_ datos. Entonces, no es buena idea hacer esa valoración sólo con los datos que usamos para entrenar. Para evitar caer en esa trampa, no queda otra alternativa que dividir el conjunto de datos que tenemos para entrenar en _dos_ subconjuntos: uno para el ajuste de los coeficientes, y otro para _probar_ (_testear_) nuestro modelo.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1632234116504,"user":{"displayName":"Ivan Meresman Higgs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtoDg2ylo0GCeSYJ0IS8owqSGaowutXNnJlTJEcA=s64","userId":"10619444747042131358"},"user_tz":180},"id":"ayaNJhcFBAbK","outputId":"5be2fa0b-1f2e-4781-8392-8a2f84c11580"},"outputs":[],"source":["#\n","# traemos (importamos) funciones de Scikit para:\n","# - separación de datos en entrenamiento y validación\n","#\n","from sklearn.model_selection import train_test_split\n","#\n","# - calcular regresión lineal\n","#\n","from sklearn.linear_model import LinearRegression\n","\n","#\n","# regresión lineal via mínimos cuadrados ordinarios (OLS)\n","#\n","# y = a_1x + a_0\n","#\n","#\n","# separamos datos en entrenamiento y validación:\n","#\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","#\n","# ajustamos con datos de entrenamiento:\n","#\n","# a) se crea el modelo lineal\n","lr = LinearRegression()\n","#\n","# b) se ajusta a los datos\n","#\n","lr.fit(X_train, y_train)\n","#\n","# vemos el resultado\n","#\n","a = lr.coef_[0]\n","b = lr.intercept_\n","print( f\"y(x) = {a:5.3f} x + {b:6.3f}\" )\n","#\n","# ajuste a datos de entrenamiento y validación\n","#\n","y_train_pred = a * X_train[:,0] + b\n","y_test_pred  = a * X_test[:,0] + b\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-SoAVFNIOl-U"},"source":["## Medida de desempeño\n","\n","Una vez que realizamos el ajuste, lo siguiente es ver qué tan bien funcionó. Lo que sigue son dos pequeñas funciones que calculan dos medidas de desempeño para este problema. La primera, la _suma de residuos cuadrados_ o _residual sum of squares_ (RSS), es una muy común.\n","\n","En estadística clásica se utiliza también la medida llamada $R^2$. Esto no es otra cosa que el RSS dividido por la _suma total de cudrados_, es decir, la suma de los valores al cuadrado de $y$ tal cual vienen. Esto nos da una medida relativa de cuánto logramos _explicar_ o _capturar_ de la función original con nuestro modelo de regresión. Si logramos un $R^2=1$, quiere decir que nuestro ajuste es perfecto. Por otro lado, mientras más bajo $R^2$, peor.\n","\n","Tal como dijimos en el apartado anterior, el RSS y el $R^2$ se evalúan tanto en el conjunto de entrenamiento como en el de validación o testeo -- si bien la última palabra tiene el de testeo, calcular ambos valores nos da más información; ya veremos cómo.\n","\n","La siguiente celda define un par de funciones para calcular el RSS y el R2, las aplica a los conjuntos de entrenamiento y test, y luego grafica los resultados.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":668},"executionInfo":{"elapsed":767,"status":"ok","timestamp":1632234117243,"user":{"displayName":"Ivan Meresman Higgs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtoDg2ylo0GCeSYJ0IS8owqSGaowutXNnJlTJEcA=s64","userId":"10619444747042131358"},"user_tz":180},"id":"X6MpczQcOn9N","outputId":"ad7b3b5e-1ccb-499f-a952-bf8680fdfc22"},"outputs":[],"source":["def rss(ypred,ytrue):\n","  '''\n","  Suma de Residuos al Cuadrado\n","  Residual Sum of Squares (RSS)\n","  '''\n","  return np.sum((ypred-ytrue)**2)\n","\n","\n","def r2(ypred,ytrue):\n","  '''\n","  score de ajuste de regresión\n","\n","     R2 = (1-RSS/TSS)\n","\n","  donde\n","\n","    RSS: suma de residuos al cuadrado: sum_i (ypred_i - ytrue_i)\n","    TSS: suma de valores al cuadrado\n","\n","  * Mientras más grande, mejor\n","  * El máximo valor es 1 (error 0)\n","  * No tiene mínimo (el modelo puede ser arbitrariamente malo)\n","\n","  esto es lo mismo que calcula la función 'score' del regresor lineal,\n","  lo implementamos de nuevo aquí por razones ilustrativas.\n","  '''\n","  RSS = np.sum((ypred-ytrue)**2)\n","  TSS = np.sum( (ytrue-np.mean(ytrue))**2 )\n","  return 1.0 - RSS/TSS\n","\n","\n","print('RSS de entrenamiento:',rss(y_train_pred,y_train))\n","print('RSS de validación   :',rss(y_test_pred, y_test))\n","print('R2  de entrenamiento:',r2(y_train_pred,y_train))\n","print('R2  de validación   :',r2(y_test_pred, y_test))\n","print(\"R2 (sklearn) de test: {:.2f}\".format(lr.score(X_train, y_train)))\n","print(\"R2 (sklearn) de val.: {:.2f}\".format(lr.score(X_test, y_test)))\n","\n","plt.plot(X_train, y_train, 'o',color=(0.5,0.5,0.5,1.0))\n","plt.plot(X_test, y_test, '*',color=(0.8,0.2,0.2,1.0))\n","plt.plot(X_train, a * X_train + b, '-',color=(0.8,0.5,0.5,1.0),lw=1)\n","\n","plt.plot()\n","plt.ylim(-2, 2)\n","plt.title('Ajuste lineal')\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.grid(True)\n","plt.savefig('ajuste_lineal.png')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"sT9wO5v1QoYO"},"source":["## Regresión polinomial\n","\n","El  ajuste $R^2=0.43$ que obtuvimos sobre los datos de _test_ es _pésimo_. Esto no debería sorprendernos: la función a ajustar está _lejos de ser una linea_. Pero eso es algo que sólo podemos saber en un caso de juguete como este; en la vida real nadie sabe de antemano qué forma tiene una función.\n","\n","Pero sabemos más: el ajuste a los datos de _entrenamiento también_ es pésimo. Eso nos indica que la función a aproximar está lejos de ser una recta.\n","\n","Qué podemos hacer? Varias cosas. Lo siguiente que vamos a probar es tal vez lo más clásico: una regresión polinomial.\n","\n","Qué es una regresión polinomial? Lo más fácil es explicarlo en el caso unidimiensional que venimos trabajando. En lugar de la relación\n","\n","$$Y = aX + b$$\n","\n","buscamos la relación:\n","\n","$$y = a_mx^m + a_{m-1}x^{m-1} + \\ldots + a_1x^1 + b$$\n","\n","Lo anterior se reduce a una regresión lineal común y corriente definiendo las siguientes _variables auxiliares_:\n","\n","$$X_i = X^i$$\n","\n","Entonces volvemos al caso general descrito al principio:\n","\n","$$Y = a_m X_m + a_{m-1}X_{m-1} + \\ldots + a_1 X_1 + b$$\n","\n","que es una regresión lineal entre la variable $Y$ y las variables $(X_1,X_2,\\ldots,X_m)$.\n","\n","El método de resolución  es _exactamente el mismo_. Lo único que hay que hacer es calcular  $x,x^2,\\ldots,x^m$ para cada dato $x$.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":583},"executionInfo":{"elapsed":643,"status":"ok","timestamp":1632234117872,"user":{"displayName":"Ivan Meresman Higgs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtoDg2ylo0GCeSYJ0IS8owqSGaowutXNnJlTJEcA=s64","userId":"10619444747042131358"},"user_tz":180},"id":"LSiZ_GIRUAg6","outputId":"af6ecde0-34bf-4600-db70-70dfbbf63241"},"outputs":[],"source":["#\n","# regresión polinomial de orden 3\n","#\n","# y = a_3x^3 + a_2x^2 + a_1x + a_0\n","#\n","# definimos características auxiliares\n","#\n","# x_1 = x\n","# x_2 = x^2\n","# x_3 = x^3\n","#\n","# regresión lineal y = f(x_1,x_2,x_3) = a_3x_3 + a_2x_2 + a_1x_1 + a_0\n","#\n","X1 = X[:,0]\n","X2 = X1**2\n","X3 = X1**3\n","X = np.zeros((N,3))\n","X[:,0] = X1\n","X[:,1] = X2\n","X[:,2] = X3\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","\n","lr.fit(X_train, y_train)\n","\n","a = lr.coef_\n","b = lr.intercept_\n","#\n","y_train_pred = a[0] * X_train[:,0] + a[1]*X_train[:,1] + a[2]*X_train[:,2] + b\n","y_test_pred  = a[0] * X_test[:,0]  + a[1]*X_test[:,1]  + a[2]*X_test[:,2]  + b\n","\n","print('R2  de entrenamiento:',r2(y_train_pred,y_train))\n","print('R2  de validación   :',r2(y_test_pred, y_test))\n","\n","plt.plot(X_train[:,0], y_train, 'o',color=(0.5,0.5,0.5,1.0))\n","plt.plot(X_test[:,0], y_test, '*',color=(0.8,0.2,0.2,1.0))\n","xaux = np.arange(0,2*np.pi,step=0.05)\n","yaux = a[0]*xaux + a[1]*xaux**2 + a[2]*xaux**3 + b\n","plt.plot(xaux,yaux, '-',color=(0.8,0.5,0.5,1.0),lw=1)\n","\n","plt.plot()\n","plt.ylim(-2, 2)\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.grid(True)\n","plt.savefig('ajuste_cubico.png')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"VRXHxYn3TkL3"},"source":["Vemos que el modelo cúbico que aplicamos logra un ajuste excelente a los datos! Pasamos a tener un $R^2$ superior a $0.85$!\n","\n","## Ajuste a datos: overfitting y underfitting\n","\n","Un modelo polinomial es más versátil que uno lineal. Una forma simple de verlo es que los modelos lineales son un caso _particular_ de los modelos polinomiales donde el único coeficiente no nulo es $a_1$.\n","\n","Usando el mismo argumento, vemos que  la _versatilidad_ de un modelo polinomial de orden más alto es siempre mayor que la de uno de orden más bajo.\n","\n","Pero lo anterior tiene un precio: al tener más parámetros, el modelo se vuelve más sensible a pequeños detalles espúreos y supérfluos de los datos de entrenamiento.\n","\n","Ya vimos que pasar de un modelo lineal a uno cúbico ayudó en este caso: el modelo lineal era _demasiado_ simple para el problema. Ahora veremos que también podemos pasarnos par el otro lado: probemos con un polinomio de orden $10$...\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":583},"executionInfo":{"elapsed":627,"status":"ok","timestamp":1632234118489,"user":{"displayName":"Ivan Meresman Higgs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtoDg2ylo0GCeSYJ0IS8owqSGaowutXNnJlTJEcA=s64","userId":"10619444747042131358"},"user_tz":180},"id":"kxwa90wtUvwp","outputId":"de5c7219-7824-454c-be85-1b9263f93543"},"outputs":[],"source":["#\n","# regresión polinomial de orden 10 (!!)\n","#\n","# y = b + \\sum_i a_i*x^i\n","#\n","# definimos características auxiliares\n","#\n","# x_i = x^i\n","# i = 1,...,10\n","#\n","# regresión lineal y = f(x_1,x_2,x_3) = a_3x_3 + a_2x_2 + a_1x_1 + a_0\n","#\n","N = 20\n","X, ytilde = onda_ruidosa(num_muestras=N,ruido=0.0)\n","X, y      = onda_ruidosa(num_muestras=N,ruido=0.3)\n","\n","P = 10\n","x = X[:,0]\n","X = np.zeros((N,P))\n","for p in range(P):\n","  X[:,p] = x**(p+1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","\n","modelo_ajustado = lr.fit(X_train, y_train)\n","\n","a = lr.coef_\n","b = lr.intercept_\n","#\n","y_train_pred = np.dot(X_train,a) + b\n","y_test_pred  = np.dot(X_test, a) + b\n","\n","print('R2  de entrenamiento:',r2(y_train_pred,y_train))\n","print('R2  de validación   :',r2(y_test_pred, y_test))\n","\n","plt.plot(X_train[:,0], y_train, 'o',color=(0.5,0.5,0.5,1.0))\n","plt.plot(X_test[:,0], y_test, '*',color=(0.8,0.2,0.2,1.0))\n","\n","x_aux, _  = onda_ruidosa(num_muestras=10*N,ruido=0.5)\n","X_aux = np.zeros((10*N,P))\n","for p in range(P):\n","  X_aux[:,p] = x_aux[:,0]**(p+1)\n","\n","y_aux = np.dot(X_aux,a) + b\n","plt.plot(X_aux[:,0], y_aux, '-',color=(0.8,0.5,0.5,1.0),lw=1)\n","\n","plt.plot()\n","plt.ylim(-2, 2)\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.grid(True)\n","plt.savefig('sobreajuste.png')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"ZaRtXIxWUlJr"},"source":["## Overfitting (sobreajuste)\n","\n","Al aplicar el modelo de orden $10$ obtuvimos un $R^2$ aún mejor en el conjunto de entrenamiento.  Sin embargo, el $R^2$ sobre el conjunto de test es $R^2=-1016$, **horrible**.  Viendo la curva anterior debería ser evidente el por qué: al tener un modelo _tan_ versátil, los parámetros se ajustaron _demasiado_ a los datos de entrenamiento, resultando en una función algo _caprichosa_ y difícil de explicar.\n","\n","Surge la pregunta entonces de cuál será el polinomio que mejor se ajuste a estos datos. Como primera aproximación al problema vamos a tomar como guía el que resulte en un mejor ajuste a los datos de _test_. Después veremos cómo mejorar este criterio.\n","\n","El siguiente código sistematiza un poco lo que ya habíamos hecho antes: la función `crear_variables`  crea y devuelve las variables auxiliares  necesarias para ajustar un polinomio de orden $P$ arbitrario.\n","\n","Luego, en el bucle `for`, probamos polinomios de distintos órdenes, evaluamos su desempeño y lo guardamos en una lista.\n","\n","Finalmente, buscamos el orden de polinomio para el cual el desempeño reportado es máximo, y declaramos ese orden como el óptimo.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":742},"executionInfo":{"elapsed":732,"status":"ok","timestamp":1632234119207,"user":{"displayName":"Ivan Meresman Higgs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtoDg2ylo0GCeSYJ0IS8owqSGaowutXNnJlTJEcA=s64","userId":"10619444747042131358"},"user_tz":180},"id":"hhwqCXUZc9VX","outputId":"12ea661a-abaa-4eca-b37a-93d9b7999bbb"},"outputs":[],"source":["#\n","# Ajuste y sobreajuste según complejidad del modelo\n","#\n","# cuál es el mejor orden de polinomio para este caso?\n","#\n","def crear_variables(x,P):\n","  '''\n","  crear variables para regresión polinómica\n","  para cada dato x, creamos un vector de P\n","  variables auxiliares\n","\n","  x -> [x, x^2, x^3, ..., x^P]\n","  '''\n","  x = x.ravel()\n","  N = len(x)\n","  Xp = np.zeros((N,P))\n","  for p in range(P):\n","    Xp[:,p] = x**(p+1)\n","  return Xp\n","\n","#\n","# creamos datos\n","#\n","N = 30\n","x, ytilde = onda_ruidosa(num_muestras=N,ruido=0.0)\n","_, y      = onda_ruidosa(num_muestras=N,ruido=0.3)\n","x = x[:,0] # primera columna (única columna)\n","\n","#\n","# división en datos de entrenamiento y datos de validación\n","#\n","X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=42)\n","\n","#\n","# repetimos el experimento para polinomios de distinto orden\n","#\n","resultados = list()\n","\n","for p in range(1,11):\n","  '''\n","  función auxiliar para repetir experimento\n","  '''\n","\n","  Xp_train = crear_variables(X_train,p)\n","  Xp_test  = crear_variables(X_test,p)\n","  #\n","  # ajuste a datos\n","  #\n","  lr.fit(Xp_train, y_train)\n","  a = lr.coef_\n","  b = lr.intercept_\n","  #\n","  # evaluación de polinomio ajustado\n","  #\n","  y_train_pred = np.dot(Xp_train,a) + b\n","  y_test_pred  = np.dot(Xp_test, a) + b\n","  r2_train = r2(y_train_pred,y_train)\n","  r2_test  = r2(y_test_pred, y_test)\n","  print(f'orden {p:2} R2 train {r2_train:0.2f} test {r2_test:0.2f}')\n","  #\n","  # terna de resultados para este caso\n","  #\n","  resultados.append( (p, r2_train,r2_test) )\n","\n","#\n","# armamos los resultados como matriz (util para manipular)\n","#\n","A = np.array(resultados)\n","mejor    = np.argmax(A[:,2]) # lugar del mayor valor de R2 en test, la tercera columna (2)\n","mejor_R2 = A[mejor,2]        # mayor valor de R2 en test, la tercera columna (2)\n","mejor_p  = A[mejor,0]        # orden de polinomio correspondiente, la primera columna\n","\n","plt.plot(A[:,0],A[:,1:])\n","plt.ylim(-1, 1)\n","plt.scatter([mejor_p],[mejor_R2],marker='o',color='r')\n","plt.xlabel(\"orden del polinomio\")\n","plt.ylabel(\"R2\")\n","plt.legend( (\"R2 train\",\"R2 test\",\"mejor\") )\n","plt.grid(True)\n","plt.savefig('ajuste_optimo.png')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"kUiQeSPVlu9Q"},"source":["## Cantidad de parámetros vs. cantidad de muestras\n","\n","El problema que vimos anteriormente es un caso particular de un problema más general y complejo que es la _selección de modelos_.  \n","\n","En ese caso, dado un conjunto de datos de prueba, vimos que el mejor polinomio que se ajustaba a los datos observados era un polinomio de orden $3$.\n","\n","Es importante entender que en la práctica no tenemos forma de saber que la función subyacente es una sinusoide o efectivamente un polinomio: simplemente resultó de nuestros experimentos que, de entre todos los poliniomios, el que mejor se ajustó fue el de orden $3$.\n","\n","Y por qué es importante hacer esa aclaración? Porque el orden del polinomio, o, en general, el modelo que mejor funciona, depende no sólo de la función subyacente (desconocida) del problema, sino de la cantidad de muestras que tenemos. Veremos qué pasa ahora, en el _mismo_ problema, si en lugar de tener $50$ muestras para entrenar tenemos sólamente $3$.\n","\n","El código de abajo repite el experimento anterior _variando_ el subconjunto de entrenamiento que usamos para ajustar los datos. Para cada subconjunto tendremos un ajuste distinto, y la idea es ver cómo varía el modelo estimado según ese subconjunto.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1689,"status":"ok","timestamp":1632234205701,"user":{"displayName":"Ivan Meresman Higgs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtoDg2ylo0GCeSYJ0IS8owqSGaowutXNnJlTJEcA=s64","userId":"10619444747042131358"},"user_tz":180},"id":"G8rcUtMdrv-k","outputId":"bc9c8c7e-4b9d-4acc-c509-504bb589d291"},"outputs":[],"source":["#\n","# Ajuste y sobreajuste según complejidad del modelo\n","#\n","# pocas muestras vs. muchas muestras\n","#\n","# cuál es el mejor orden de polinomio para este caso?\n","#\n","#\n","N = 30\n","x, ytilde = onda_ruidosa(num_muestras=N,ruido=0.0)\n","_, y      = onda_ruidosa(num_muestras=N,ruido=0.3)\n","x = x[:,0] # primera columna (única columna)\n","\n","x_aux    = np.arange(0,2*np.pi,step=0.1)\n","\n","#\n","# repetimos el experimento para distintos subconjuntos de muestras\n","# de entrenamiento\n","#\n","# la semilla es un número entero elegido arbitrariamente que sirve\n","# para dar un estado inicial al generador de números pseudo-aleatorios\n","# (para cada número entero se genera una secuencia de números pseudo-aleatorios\n","# particular y única.)\n","#\n","handles = list()\n","legends = list()\n","\n","for semilla in (1234,4321,8008):\n","  #\n","  # MUY POCAS MUESTRAS DE ENTRENAMIENTO!\n","  # El mínimo posible para ajustar un polinomio de orden p es p+1!\n","  #\n","  #\n","  X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=4, random_state=semilla)\n","  #\n","  # repetimos el experimento para polinomios de orden 1 y 3\n","  #\n","  for p in (0,3):\n","\n","    #\n","    # ajuste a datos\n","    #\n","    if p > 0:\n","      Xp_train = crear_variables(X_train,p)\n","      Xp_test  = crear_variables(X_test,p)\n","      Xp_aux   = crear_variables(x_aux,p)\n","      modelo_ajustado = lr.fit(Xp_train, y_train)\n","      a = lr.coef_\n","      b = lr.intercept_\n","      y_train_pred = np.dot(Xp_train,a) + b\n","      y_test_pred  = np.dot(Xp_test, a) + b\n","      y_aux = np.dot(Xp_aux,a) + b\n","    else:\n","      Xp_train = crear_variables(X_train,1)\n","      Xp_test  = crear_variables(X_test,1)\n","      Xp_aux   = crear_variables(x_aux,1)\n","      b = np.mean(y_train)\n","      y_train_pred = b * np.ones(Xp_train.shape[0])\n","      y_test_pred  = b * np.ones(Xp_test.shape[0])\n","      y_aux        = b * np.ones(Xp_aux.shape[0])\n","\n","    r2_train = r2(y_train_pred,y_train)\n","    r2_test  = r2(y_test_pred, y_test)\n","    print(f'orden {p:2} R2 train {r2_train:0.2f} test {r2_test:0.2f}')\n","    #\n","    #\n","    #\n","    plt.figure(p)\n","    plt.plot(x_aux,y_aux,color=(0.5,0.5,0.5,1.0))\n","    plt.scatter(Xp_test[:,0],y_test,color=(0,0,0,0.1))\n","    h = plt.scatter(Xp_train[:,0],y_train)\n","    if p > 0:\n","      handles.append(h)\n","      legends.append(f'train s={semilla}')\n","\n","\n","for p in (0,3):\n","  plt.figure(p)\n","  plt.ylim(-3,3)\n","  plt.xlabel(f\"polinomio de orden {p}\")\n","  plt.ylabel(\"R2\")\n","  plt.grid(True)\n","  plt.legend(handles,legends)\n","  plt.savefig(f'pocas_muestras_p{p}.png')\n","\n","plt.show()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BR5e2sl1n2rG"},"source":["## Interpretación de los resultados anteriores\n","\n","Como resultado tenemos dos gráficas: en cada una de ellas tenemos $3$ ajustes distintos derivados de $3$ subconjuntos de entrenamiento distintos (los puntos azul, naranja y verde).\n","\n","La primera gráfica corresponde a un ajuste lineal de orden **cero**, es decir, $y = b$ (la variable $X$ no se usa); ese es el modelo lineal más sencillo posible: ajustamos a la función con un valor constante.\n","\n","La segunda gráfica muestra el ajuste de un polinomio de orden $3$, tal como el que usamos en el estudio anterior.\n","\n","Podemos ver tres cosas:\n","\n","*   El polinomio de orden $3$ da _perfecto_ en el conjunto de _entrenamiento_\n","*   El polinomio de orden $3$  da _mucho peor_ en el conjunto de _test_\n","*   El $R^2$ del polinomio de orden $3$ da _mucho_ peor que el modelo de orden $0$ sobre el conjunto de _test_.\n","*   Ninguno de los ajustes del polinomio de orden $3$ se parece ni de asomo al modelo aprendido cuando teníamos $50$ muestras de entrenamiento\n","\n","La verdad es que todo polinomio de orden $p$ es capaz de ajustarse _perfectamente_ a $p+1$ puntos, y por eso el ajuste es siempre perfecto. Este es el summum del sobreajuste, y la consecuencia esperada es que la generalización es pésima: anda muy mal con datos que no sean los de entrenamiento.\n","\n","Sorprendentemente, frente a tan pocos datos, lo mejor que podemos hacer es aproximar la función con una constante. C'est la vie.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sD279bPZppIL"},"source":["# Regularización\n","\n","Del principio de este Notebook podría quedar la idea de que los modelos lineales son muy simples.\n","\n","Sin embargo, en el fondo, todos los modelos que vimos anteriormente son lineales, sólo que expresados en un espacio de dimensión más alta: para regresión polinomial, transformamos cada dato escalar $x$ en un vector multidimensional de dimensión $p$.\n","\n","Vimos también que podemos llegar a tener modelos muy complejos con esta metodología.\n","\n","En resumen, lo que vimos, de manera un poco indirecta, es que los modelos lineales pueden llegar a ser muy complejos, incluso _demasiado complejos_, cuando la cantidad de variables (y por ende la dimensión del problema) es grande.\n","\n","Vimos también que la complejidad inherente del modelo  (dicho de manera simple, la cantidad de parámetros) juega un rol importante en su capacidad de ajuste (y peligro de sobreajuste) a los datos.\n","\n","Lo que vamos a ver ahora es cómo _reducir_ o _controlar_ la complejidad de un modelo lineal en dimensiones altas. Esto se llama _regularización_, y tiene varios sabores.\n","\n","La estrategia general consiste en _promover_, _fomentar_ o bien _restringir_ los valores de lo coeficientes estimados $a_1,a_2,\\ldots,a_m$ de manera de que cumplan cierto criterio de _parsimonia_.\n","\n","Los ejemplos que vamos a ver abajo incluyen al llamado _Ridge Regression_, cuya premisa es que los coeficientes _no sean demasiado grandes_, y el _LASSO_, históricamente mucho más reciente (mediados de los 1990's).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8025,"status":"ok","timestamp":1632234601143,"user":{"displayName":"Ivan Meresman Higgs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtoDg2ylo0GCeSYJ0IS8owqSGaowutXNnJlTJEcA=s64","userId":"10619444747042131358"},"user_tz":180},"id":"LkDdV32-AE7p","outputId":"6f948582-286b-4634-833d-2f2a130bbfd6"},"outputs":[],"source":["\n","#\n","# - separación de datos en entrenamiento y validación\n","#\n","from sklearn.model_selection import train_test_split\n","#\n","# - calcular regresión lineal\n","#\n","from sklearn.linear_model import LinearRegression,Ridge,Lasso\n","\n","#X,y = boston\n","N= 70\n","x, ytilde = onda_ruidosa(num_muestras=N,ruido=0.0)\n","_, y      = onda_ruidosa(num_muestras=N,ruido=1.0)\n","X = x.reshape(-1,1)\n","n,m = X.shape\n","print(f\"datos: {n} muestras de dimensión {m} cada una\")\n","\n","#\n","# regresión lineal via mínimos cuadrados ordinarios (OLS)\n","#\n","# y = a_1x + a_0\n","#\n","#\n","# separamos datos en entrenamiento y validación:\n","#\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=10)\n","#\n","# lista de modelos que vamos a probar\n","#\n","modelos = {\n","    'Lineal     ': LinearRegression(),  # modelo lineal\n","    'Ridge@0.010':Ridge(alpha=0.010),   # Ridge, poca reg.\n","    'Ridge@0.100':Ridge(alpha=0.100),   # Ridge, ideal\n","    'Ridge@1.000':Ridge(alpha=1.000),   # Ridge, mucha reg.\n","    'Ridge@10.00':Ridge(alpha=10.00),   # Ridge, mucha reg.\n","    'Lasso@.0001':Lasso(alpha=0.0001,max_iter=100000),  # LASSO, poca.\n","    'Lasso@0.001':Lasso(alpha=0.001,max_iter=100000),  # LASSO, poca.\n","    'Lasso@0.01':Lasso(alpha=0.01,max_iter=10000),  # LASSO, ideal\n","    'Lasso@0.1':Lasso(alpha=0.1,max_iter=10000)   # LASSO, mucha\n","}\n","#\n","# repetimos lo siguiente para cada modelo:\n","#\n","for nombre in modelos.keys():\n","  modelo = modelos[nombre]\n","  # ajuste a los datos:\n","  modelo = modelo.fit(X_train, y_train)\n","  # calidad de ajuste:\n","  R2train = modelo.score(X_train,y_train)\n","  R2test  = modelo.score(X_test,y_test)\n","  print(f\"{nombre}: train {R2train:5.3f} test {R2test:5.3f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"k1llN-pdzUqD"},"source":["### Observaciones\n","\n","De lo anterior observamos varias cosas:\n","\n","*   Con el parámetro justo, podemos mejorar significativamente el desempeño en _test_ del modelo (que es el que importa)\n","*   Si la regularización es muy baja, el desempeño se acerca al de mínimos cuadrados (de hecho es idéntico cuando $\\alpha=0$ en ambos casos)\n","*   Si la regularización es demasiada, el desempeño cae tanto en entrenamiento como en validación: _underfitting_!\n","*   El valor del parámetro óptimo es muy distinto según el tipo de regularización!\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","# Regresión logística\n","\n","La regresión logística es utilizada cuando lo que se desea inferir no es una magnitud (por ejemplo, una temperatura) sino un valor de tipo verdadero-falso. Esto se puede utilizar para detección (presencia-no presencia) o bien generalizar a problemas de clasificación de datos en $c$ clases en donde lo que se obtiene es un conjunto de probabilides $(p_1,p_2,\\ldots,p_c)$ de que un cierto dato de entrada $x$ pertenezca a cada una de las categorías $(1,2,\\ldots,c)$.\n","\n","Todo lo que vimos antes se aplica a este caso: overfitting, underfitting, regularización. \n","\n","## Problema de ejemplo: Cancer de mama\n","\n","El objetivo del siguiente problema es predecir si una persona tiene o no tiene cancer de mama según un conjunto de variables medidas.\n","El conjunto de datos es parte del paquete `Scikit`. Además de los datos en sí, la biblioteca contiene información sobre los datos, su origen, contenido, etc. Esto lo mostramos primero aquí abajo."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","#\n","# datasets de prueba que vienen con la biblioteca Scikit\n","#\n","from sklearn import datasets\n","\n","#\n","# cargamos los datos\n","#\n","cancer = datasets.load_breast_cancer()\n","print(cancer['DESCR'])\n","X = cancer['data']\n","y = cancer['target']\n","n = len(y)\n","ntest = (n*20)//100 # 20% para test\n","ntrain = n - ntest\n","#\n","# dividimos en entrenamiento y validación \n","#\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=ntest)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Entrenamiento y despliegue de resultados"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#\n","# modelo de regresión logística \n","#\n","from sklearn.linear_model import LogisticRegression\n","#\n","# creamos el modelo con regularización L2 penalizada, coeficiente d epenalización C = 0.1\n","#\n","model       = LogisticRegression(C=0.1,penalty='l2',max_iter=10000)\n","#\n","# ajustamos el modelo a los datos de entrenamiento\n","#\n","model       = model.fit(X_train, y_train)\n","#\n","# medimos la calidad del ajuste en entrenamiento y test\n","#\n","train_score = model.score(X_train, y_train)\n","test_score  = model.score(X_test, y_test)\n","print(f\"LOGREG score train: {train_score:.2f} test: {test_score:.2f}\")\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
