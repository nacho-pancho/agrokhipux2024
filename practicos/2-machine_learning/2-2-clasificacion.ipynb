{"cells":[{"cell_type":"markdown","metadata":{"id":"4ubr1U7IAVDn"},"source":["# Práctico 2-2: Clasificación \n","\n","En este Notebook vamos a explorar brevemente algunos de los métodos más conocidos y conceptualmente sencillos para aprender y aplicar un sistema de aprendizaje automático para imputar la clase de una muestra.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Wx827YfL8fdK"},"source":["### Preámbulo\n","\n","*   Importamos los paquetes en el código (ya instalados anteriormente)\n","*   Configuramos algunos parámetros de visualización (tamaño de letra, gráficas, et.c)\n","*   Cargamos datos para experimentar"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":506},"executionInfo":{"elapsed":1969,"status":"ok","timestamp":1696355272498,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"wKKQ--i1-zm2","outputId":"d9e3c87d-00de-449c-d10c-dcd375ef897d"},"outputs":[],"source":["\n","#\n","# importación\n","#\n","import matplotlib as mpl\n","import matplotlib.cm as cm\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import numpy.random as rng\n","\n","mpl.rcParams['figure.dpi'] = 100\n","mpl.rcParams['savefig.dpi'] = 150\n","\n","mpl.rcParams['font.size'] = 10\n","mpl.rcParams['legend.fontsize'] = 'medium'\n","mpl.rcParams['figure.titlesize'] = 'medium'\n","\n","def fronteras_sinuosas(num_samples=30,semilla=356):\n","  rng.seed(semilla)\n","  X = np.pi*(1-2*rng.rand(num_samples,2))\n","  y = 1*(X[:,1] > 2*np.sin(X[:,0]))\n","  return X,y\n","#\n","# dataset sintético para clasificación\n","#\n","X, y = fronteras_sinuosas(num_samples=50,semilla=3356)\n","\n","# plot dataset\n","plt.scatter(X[:,0],X[:,1],s=50,c=y,cmap=cm.bwr)\n","plt.xlabel(\"Primera característica\")\n","plt.ylabel(\"Segunda característica\")\n","plt.title(\"Muestras del problema de clasificación 2D sintético\")\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZhvnFJ8uAk6R"},"source":["# Vecinos más cercanos\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":944},"executionInfo":{"elapsed":2918,"status":"ok","timestamp":1696355275414,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"lsHIUM7vVd-L","outputId":"845628cf-9f80-4f0f-bfca-2dd62e6e5f60"},"outputs":[],"source":["#\n","# importamos módulos necesarios para esta parte\n","#\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from matplotlib.colors import ListedColormap\n","#\n","# dividimos en conjuntos de entrenamiento y de validación\n","#\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=20, random_state=1716)\n","#\n","# creamos el clasificador de vecinos más cercanos y lo configuramos\n","# para que use 1 vecino (k=1)\n","#\n","K = 1\n","knn = KNeighborsClassifier(n_neighbors=K)\n","#\n","# ajustamos a datos\n","#\n","knn = knn.fit(X_train, y_train)\n","#\n","# inferimos, imputamos o \"predecimos\" la clase correspondiente a cada\n","# punto de validación\n","#\n","y_pred_test = knn.predict(X_test)\n","#\n","# calculamos índice de desempeño\n","# es simplemente el porcentaje de aciertos a la clase\n","#\n","score = np.sum(y_pred_test == y_test)/len(y_test)\n","print(f\"Porcentaje de acierto: {score:5.2f}\")\n","#\n","# veamos la clasificación obtenida\n","#\n","newcolors = [(*c[:3],0.3) for c in cm.bwr(np.linspace(0, 1, 256))]\n","bwr_transp = ListedColormap(newcolors)\n","#plt.scatter(X[:,0],X[:,1],color=(0.0,0.0,0.0,0.25),s=20)\n","plt.scatter(X_train[:,0],X_train[:,1],marker='o',c=y_train,s=30,cmap=bwr_transp)\n","plt.scatter(X_test[:,0], X_test[:,1],marker='*',c=y_pred_test,s=50,cmap=cm.bwr)\n","plt.title('Clasificación de puntos de prueba (estrella)')\n","plt.xlabel('Dato 1')\n","plt.ylabel('Dato 2')\n","plt.show()\n","#\n","# mostrar región de clasificación\n","#\n","def pintar_regiones(X,y,clasificador,res=100):\n","  '''\n","  pintamos la región de decisión\n","  barremos todo el espacio con una grilla fina y pintamos cada punto\n","  con la clase asignada.\n","  '''\n","  x1s = np.linspace(-np.pi,np.pi,100)\n","  x2s = np.linspace(-np.pi,np.pi,100)\n","  ns = len(x1s)*len(x2s)\n","  X_aux = np.zeros((ns,2))\n","  i = 0\n","  for x1 in x1s:\n","    for x2 in x2s:\n","      X_aux[i,0] = x1\n","      X_aux[i,1] = x2\n","      i += 1\n","  y_aux = clasificador.predict(X_aux)\n","  #\n","  # grilla fina:\n","  #\n","  plt.scatter(X_aux[:,0],X_aux[:,1],marker='o',edgecolors='none',c=y_aux,s=10,cmap=bwr_transp)\n","  #\n","  # puntos de referencia\n","  #\n","  plt.scatter(X[:,0],X[:,1],marker='o',c=y,s=50,cmap=cm.bwr)\n","  plt.title('Región de decisión')\n","  plt.xlabel('Dato 1')\n","  plt.ylabel('Dato 2')\n","  plt.show()\n","#\n","# ejecutamos función\n","#\n","pintar_regiones(X_train,y_train,knn)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489},"executionInfo":{"elapsed":5603,"status":"ok","timestamp":1696355281014,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"S2LvIHEdgl6f","outputId":"87b58988-b01b-4c0e-ca7e-d3ad995c0499"},"outputs":[],"source":["#\n","# K = 3\n","#\n","K =3\n","knn = KNeighborsClassifier(n_neighbors=K)\n","#\n","# ajustamos a datos\n","#\n","knn = knn.fit(X_train, y_train)\n","y_pred_test = knn.predict(X_test)\n","#\n","# calculamos índice de desempeño\n","#\n","score = np.sum(y_pred_test == y_test)/len(y_test)\n","print(f\"Porcentaje de acierto: {score:5.2f}\")\n","#\n","# pintamos\n","#\n","pintar_regiones(X_train,y_train,knn)\n"]},{"cell_type":"markdown","metadata":{"id":"QD4zF9N_n05k"},"source":["## Clasificadores lineales\n","\n","### Regresión logística\n","\n","A pesar de su nombre, la regresión logística es un método de clasificacion.\n","La ambigüedad surge de que, si bien _es_ un método de regresión al igual que la regresión lineal que vimos en el notebook anterior, el _objetivo_ de la _Regresión Logística_ es la clasificación, es decir, la variable de respuesta $y$ es una _clase_ y no una magnitud continua, como suele ser el caso de la regresión tradicional.\n","\n","A pesar de su sencillez, este mecanismo de clasificación es muy versátil, fácil de entender, y efectivo en muchos escenarios.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1696355281015,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"87vRXW-OoBQE","outputId":"82efc270-1f16-415f-c2e7-86f5e7be037a"},"outputs":[],"source":["#\n","# regresión logística\n","#\n","from sklearn.linear_model import LogisticRegression\n","\n","model       = LogisticRegression(C=0.1,penalty='l2')\n","model       = model.fit(X_train, y_train)\n","train_score = model.score(X_train, y_train)\n","test_score  = model.score(X_test, y_test)\n","print(f\"LOGREG score train: {train_score:.2f} test: {test_score:.2f}\")\n","\n","\n","pintar_regiones(X_train,y_train,model)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iv3XUD6beOhd"},"source":["## Support Vector Machines\n","\n","Estos clasificadores lineales, de nombre bastante críptico incluso para expertos, dividen el espacio en regiones mediante un hiperplano (de ahí lo lineal). A diferencia de la regresión logística, sin embargo, el hiperplano no se construye de manera de ajustarse a una respuesta $y$, sino que se utiliza un criterio de _margen de seguridad_. La idea es que la frontera tenga cierto _margen de separación_ con respecto a los puntos de entrenamiento, es decir, se trata de que los puntos de entrenamiento no queden demasiado cerca del borde.\n","La forma en que esto se hace no es del todo trivial y no aporta mucho explicar aquí cómo funciona. Lo único que importa saber es que el parámetro de _regularización_ en este caso está asociado a la importancia que se le da a este margen en el ajuste a datos.\n","\n","La forma de aplicar este método es idéntica a otras que vimos antes; sólo hay que cambiar el modelo:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1696355281015,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"JBQgH_6vfRoM","outputId":"8038da1c-6103-4f5f-a454-b274e50f7aa3"},"outputs":[],"source":["#\n","# SVM = Support Vector Machines\n","# SVC = Support Vector Classifier\n","#\n","from sklearn.svm import LinearSVC\n","\n","model       = LinearSVC(C=0.01,max_iter=100000,dual=\"auto\").fit(X_train, y_train)\n","train_score = model.score(X_train, y_train)\n","test_score  = model.score(X_test, y_test)\n","print(f\"SVC score train: {train_score:.2f} test: {test_score:.2f}\")\n","\n","pintar_regiones(X_train,y_train,model)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Ejercicio\n","\n","Complete la celda de abajo "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
