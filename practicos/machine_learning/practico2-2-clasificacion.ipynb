{"cells":[{"cell_type":"markdown","metadata":{"id":"4ubr1U7IAVDn"},"source":["# Práctico 2-2: Clasificación \n","\n","En este Notebook vamos a explorar brevemente algunos de los métodos más conocidos y conceptualmente sencillos para aprender y aplicar un sistema de aprendizaje automático para imputar la clase de una muestra.\n","\n","### Preparación del entorno\n","\n","Como casi siempre, hay que cargar algunas cositas antes de trabajar.\n","En este caso vamos a instalar el paquete `mglearn`, complemento del libro\n","de Müller & Guido que seguimos en el curso.\n","También vamos a usar el paquete `dtreeviz` para visualización.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17368,"status":"ok","timestamp":1696355270543,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"KdbaD26j8bt4","outputId":"48726133-b397-4988-9053-1da9e53c51d1"},"outputs":[],"source":["#\n","# INSTALACIÓN\n","#\n","# Esto instala paquetes en la máquina virtual que nos ofrece Google Colab.\n","#\n","# paquete con utilidades y código del libro\n","# de Andreas Müller & Sarah Guido\n","#\n","!pip install --quiet mglearn\n","#\n","# visualización de árboles de decisión\n","#\n","!pip install --quiet dtreeviz\n"]},{"cell_type":"markdown","metadata":{"id":"Wx827YfL8fdK"},"source":["# Preámbulo\n","\n","*   Importamos los paquetes en el código (ya instalados anteriormente)\n","*   Configuramos algunos parámetros de visualización (tamaño de letra, gráficas, et.c)\n","*   Cargamos datos para experimentar"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":506},"executionInfo":{"elapsed":1969,"status":"ok","timestamp":1696355272498,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"wKKQ--i1-zm2","outputId":"d9e3c87d-00de-449c-d10c-dcd375ef897d"},"outputs":[],"source":["\n","#\n","# importación\n","#\n","import matplotlib as mpl\n","import matplotlib.cm as cm\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import numpy.random as rng\n","\n","mpl.rcParams['figure.dpi'] = 100\n","mpl.rcParams['savefig.dpi'] = 150\n","\n","mpl.rcParams['font.size'] = 10\n","mpl.rcParams['legend.fontsize'] = 'medium'\n","mpl.rcParams['figure.titlesize'] = 'medium'\n","\n","def fronteras_sinuosas(num_samples=30,semilla=356):\n","  rng.seed(semilla)\n","  X = np.pi*(1-2*rng.rand(num_samples,2))\n","  y = 1*(X[:,1] > 2*np.sin(X[:,0]))\n","  return X,y\n","#\n","# dataset sintético para clasificación\n","#\n","X, y = fronteras_sinuosas(num_samples=50,semilla=3356)\n","\n","# plot dataset\n","plt.scatter(X[:,0],X[:,1],s=50,c=y,cmap=cm.bwr)\n","plt.xlabel(\"Primera característica\")\n","plt.ylabel(\"Segunda característica\")\n","plt.title(\"Muestras del problema de clasificación 2D sintético\")\n","plt.show()\n","\n","#\n","# dataset real:\n","# cancer de mama ('cancer')\n","#\n","from sklearn import datasets\n","\n","cancer = datasets.load_breast_cancer()\n","print(\"cancer.keys(): \\n{}\".format(cancer.keys()))\n","#\n","# dataset real:\n","# especies de iris ('iris)\n","#\n","iris = datasets.load_iris()\n","#X = iris.data\n","#y = iris.target\n"]},{"cell_type":"markdown","metadata":{"id":"ZhvnFJ8uAk6R"},"source":["# Vecinos más cercanos\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":944},"executionInfo":{"elapsed":2918,"status":"ok","timestamp":1696355275414,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"lsHIUM7vVd-L","outputId":"845628cf-9f80-4f0f-bfca-2dd62e6e5f60"},"outputs":[],"source":["#\n","# importamos módulos necesarios para esta parte\n","#\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from matplotlib.colors import ListedColormap\n","#\n","# dividimos en conjuntos de entrenamiento y de validación\n","#\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=20, random_state=1716)\n","#\n","# creamos el clasificador de vecinos más cercanos y lo configuramos\n","# para que use 1 vecino (k=1)\n","#\n","K = 1\n","knn = KNeighborsClassifier(n_neighbors=K)\n","#\n","# ajustamos a datos\n","#\n","knn = knn.fit(X_train, y_train)\n","#\n","# inferimos, imputamos o \"predecimos\" la clase correspondiente a cada\n","# punto de validación\n","#\n","y_pred_test = knn.predict(X_test)\n","#\n","# calculamos índice de desempeño\n","# es simplemente el porcentaje de aciertos a la clase\n","#\n","score = np.sum(y_pred_test == y_test)/len(y_test)\n","print(f\"Porcentaje de acierto: {score:5.2f}\")\n","#\n","# veamos la clasificación obtenida\n","#\n","newcolors = [(*c[:3],0.3) for c in cm.bwr(np.linspace(0, 1, 256))]\n","bwr_transp = ListedColormap(newcolors)\n","#plt.scatter(X[:,0],X[:,1],color=(0.0,0.0,0.0,0.25),s=20)\n","plt.scatter(X_train[:,0],X_train[:,1],marker='o',c=y_train,s=30,cmap=bwr_transp)\n","plt.scatter(X_test[:,0], X_test[:,1],marker='*',c=y_pred_test,s=50,cmap=cm.bwr)\n","plt.title('Clasificación de puntos de prueba (estrella)')\n","plt.xlabel('Dato 1')\n","plt.ylabel('Dato 2')\n","plt.show()\n","#\n","# mostrar región de clasificación\n","#\n","def pintar_regiones(X,y,clasificador,res=100):\n","  '''\n","  pintamos la región de decisión\n","  barremos todo el espacio con una grilla fina y pintamos cada punto\n","  con la clase asignada.\n","  '''\n","  x1s = np.linspace(-np.pi,np.pi,100)\n","  x2s = np.linspace(-np.pi,np.pi,100)\n","  ns = len(x1s)*len(x2s)\n","  X_aux = np.zeros((ns,2))\n","  i = 0\n","  for x1 in x1s:\n","    for x2 in x2s:\n","      X_aux[i,0] = x1\n","      X_aux[i,1] = x2\n","      i += 1\n","  y_aux = clasificador.predict(X_aux)\n","  #\n","  # grilla fina:\n","  #\n","  plt.scatter(X_aux[:,0],X_aux[:,1],marker='o',edgecolors='none',c=y_aux,s=10,cmap=bwr_transp)\n","  #\n","  # puntos de referencia\n","  #\n","  plt.scatter(X[:,0],X[:,1],marker='o',c=y,s=50,cmap=cm.bwr)\n","  plt.title('Región de decisión')\n","  plt.xlabel('Dato 1')\n","  plt.ylabel('Dato 2')\n","  plt.show()\n","#\n","# ejecutamos función\n","#\n","pintar_regiones(X_train,y_train,knn)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489},"executionInfo":{"elapsed":5603,"status":"ok","timestamp":1696355281014,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"S2LvIHEdgl6f","outputId":"87b58988-b01b-4c0e-ca7e-d3ad995c0499"},"outputs":[],"source":["#\n","# K = 3\n","#\n","K =3\n","knn = KNeighborsClassifier(n_neighbors=K)\n","#\n","# ajustamos a datos\n","#\n","knn = knn.fit(X_train, y_train)\n","y_pred_test = knn.predict(X_test)\n","#\n","# calculamos índice de desempeño\n","#\n","score = np.sum(y_pred_test == y_test)/len(y_test)\n","print(f\"Porcentaje de acierto: {score:5.2f}\")\n","#\n","# pintamos\n","#\n","pintar_regiones(X_train,y_train,knn)\n"]},{"cell_type":"markdown","metadata":{"id":"QD4zF9N_n05k"},"source":["## Clasificadores lineales\n","\n","### Regresión logística\n","\n","A pesar de su nombre, la regresión logística es un método de clasificacion.\n","La ambigüedad surge de que, si bien _es_ un método de regresión al igual que la regresión lineal que vimos en el notebook anterior, el _objetivo_ de la _Regresión Logística_ es la clasificación, es decir, la variable de respuesta $y$ es una _clase_ y no una magnitud continua, como suele ser el caso de la regresión tradicional.\n","\n","A pesar de su sencillez, este mecanismo de clasificación es muy versátil, fácil de entender, y efectivo en muchos escenarios.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1696355281015,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"87vRXW-OoBQE","outputId":"82efc270-1f16-415f-c2e7-86f5e7be037a"},"outputs":[],"source":["#\n","# regresión logística\n","#\n","from sklearn.linear_model import LogisticRegression\n","\n","model       = LogisticRegression(C=0.1,penalty='l2')\n","model       = model.fit(X_train, y_train)\n","train_score = model.score(X_train, y_train)\n","test_score  = model.score(X_test, y_test)\n","print(f\"LOGREG score train: {train_score:.2f} test: {test_score:.2f}\")\n","\n","\n","pintar_regiones(X_train,y_train,model)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iv3XUD6beOhd"},"source":["## Support Vector Machines\n","\n","Estos clasificadores lineales, de nombre bastante críptico incluso para expertos, dividen el espacio en regiones mediante un hiperplano (de ahí lo lineal). A diferencia de la regresión logística, sin embargo, el hiperplano no se construye de manera de ajustarse a una respuesta $y$, sino que se utiliza un criterio de _margen de seguridad_. La idea es que la frontera tenga cierto _margen de separación_ con respecto a los puntos de entrenamiento, es decir, se trata de que los puntos de entrenamiento no queden demasiado cerca del borde.\n","La forma en que esto se hace no es del todo trivial y no aporta mucho explicar aquí cómo funciona. Lo único que importa saber es que el parámetro de _regularización_ en este caso está asociado a la importancia que se le da a este margen en el ajuste a datos.\n","\n","La forma de aplicar este método es idéntica a otras que vimos antes; sólo hay que cambiar el modelo:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1696355281015,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"JBQgH_6vfRoM","outputId":"8038da1c-6103-4f5f-a454-b274e50f7aa3"},"outputs":[],"source":["#\n","# SVM = Support Vector Machines\n","# SVC = Support Vector Classifier\n","#\n","from sklearn.svm import LinearSVC\n","\n","model       = LinearSVC(C=0.01,max_iter=100000).fit(X_train, y_train)\n","train_score = model.score(X_train, y_train)\n","test_score  = model.score(X_test, y_test)\n","print(f\"SVC score train: {train_score:.2f} test: {test_score:.2f}\")\n","\n","pintar_regiones(X_train,y_train,model)"]},{"cell_type":"markdown","metadata":{"id":"g8Jl4QatoECY"},"source":["## Árboles de decisión\n","\n","Estos modelos pueden verse como un juego de preguntas. Para clasificar un dato se realizan una serie de preguntas; la respuesta a cada pregunta determina una bifurcación en un árbol (la primera pregunta corresponde a la raíz del árbol). Una bifurcación puede llevar a otra bifurcación (otra pregunta), o puede ser suficiente para determinar la clase, en cuyo caso se llega a una _hoja_.\n","\n","Existen diferentes métodos para determinar la secuencia de preguntas que constituye el proceso de clasificación en un modelo de este tipo. Abajo vemos cómo ajustar un árbol para clasificar flores de iris. Una ventaja de este tipo de modelos es que suelen ser fácilmente interpretables por un humano, lo que siempre es deseable.\n","\n","Al final vemos varias utilidades gráficas para mostrar este tipo de árboles de manera interpretable.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3974,"status":"ok","timestamp":1696355284981,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"iBqiZYJc-OZS","outputId":"93c09af9-610d-4f00-99e9-64f758aa7653"},"outputs":[],"source":["#\n","# arboles de decisión\n","#\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import tree\n","\n","# Prepare the data data\n","# Fit the classifier with default hyper-parameters\n","clf = DecisionTreeClassifier(random_state=1234)\n","X = iris.data\n","y = iris.target\n","model = clf.fit(X, y)\n","\n","#\n","# visualizacion con SKLearn\n","#\n","#fig = plt.figure(figsize=(25,20))\n","#_ = tree.plot_tree(clf,\n","#                   feature_names=iris.feature_names,\n","#                   class_names=iris.target_names,\n","#                   filled=True)\n","#\n","#fig.savefig(\"decistion_tree.png\")\n","#\n","# visualizacion básica con Graphviz\n","#\n","#print(X.shape)\n","#import graphviz\n","#dot_data = tree.export_graphviz(clf, out_file=None,\n","#                                feature_names=iris.feature_names,\n","#                                class_names=iris.target_names,\n","#                                filled=True)\n","#\n","# Draw graph\n","#graph = graphviz.Source(dot_data, format=\"png\")\n","#graph.render(\"decision_tree_graphivz\")\n","\n","#\n","# visualización con dtreeviz\n","#\n","# from dtreeviz.trees import dtreeviz # remember to load the package\n","\n","import dtreeviz\n","\n","viz_model = dtreeviz.model(clf, X, y,\n","                target_name=\"target\",\n","                feature_names=iris.feature_names,\n","                class_names=list(iris.target_names))\n","\n","viz = viz_model.view()     # render as SVG into internal object\n","\n","viz.save(\"decision_tree.svg\")\n","\n","from IPython.display import SVG\n","SVG(\"decision_tree.svg\")"]},{"cell_type":"markdown","metadata":{"id":"EdtbQJC9iYeZ"},"source":["El resultado del último arbol puede verse aquí:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":565},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1696355284981,"user":{"displayName":"Ignacio Francisco Ramírez","userId":"07143032839839921583"},"user_tz":180},"id":"4r5W1xiT_3wj","outputId":"5ea13904-cf70-42f7-aa51-58ec3ba101c8"},"outputs":[],"source":["from IPython.display import SVG\n","SVG(\"decision_tree.svg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHQgqNRx2QSF"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
